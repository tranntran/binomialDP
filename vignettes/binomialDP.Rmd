---
title: "binomialDP: Differentially Private Inference for Binomial Data"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{binomialDP: Differentially Private Inference for Binomial Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
`binomialDP` package aids the process of analyzing differentially private (DP) data by offering a tool to perform statistical inference on differentially private binomial data. This package includes functions for the Tulap distributions, DP-UMP tests, private p-values, and private confidence intervals for binomial data. 

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

## The Truncated-Uniform-Laplace (Tulap) Distrubution
The distribution $\small Tulap(m, b, 0)$ is obtained by adding discrete Laplace noise $\small L \sim DLap(e^{-\epsilon})$ and continuous uniform noise $\small U \sim Unif(-1/2, 1/2)$ to the value m. The distribution $\small Tulap(m,b,q)$ is obtained by truncating between the $\small (q/2)^{th}$ and the $\small (1-q/2)^{th}$ quantiles of the $\small Tulap(m,b,0)$ distribution. 

Similar to other distribution functions, the `ptulap()` function gives the distribution function, and `rtulap()` generates random sample from this distribution. For example, to sample 10,000 observations from Tulap distribution, with the median of 30, $\small b = e^{-1}$, and $\small q = 0.06$:

```{r, fig.width = 4.5, fig.height = 3.5, fig.align = 'center'}
library(binomialDP)
set.seed(1001)
rand = rtulap(n = 10000, m = 30, b = exp(-1), q = 0.06)
plot(density(rand), main = "Random Sample of 10000 \n from Tulap Distribution", xlab = "Value")
```

The figure confirms that a Tulap random variable is continuous and symmetric around the median. When q = 0, the distribution is not truncated.

## Simulated Data

First, we obtain the original data set by sampling from the binomial distribution with `rbinom()`. Then, we make it differentially private by adding noise from the Tulap distribution through the function `rtulap()`. The latter data is the data we will perform statistical testing on. 

Let $\small X \sim Bin(n,\theta)$ where $\small n = 30$ and $\small \theta = 0.4$, but the value of $\small \theta$ is supposed to be unknown. Also, with $\small \epsilon = 1$ and $\small \delta = 0.01$, we obtain:
$$\small b = e^{-\epsilon} = e^{-1} ~~~~~~~~~~~~~~~~~~ q = \frac{2\delta b}{1-b+2\delta b} = \frac{2(0.01)e^{-1}}{1-e^{-1}+2(0.01)e^{-1}}$$

We obtain the differentially private binomial data $\small Z$ by adding noise from $\small Tulap(0,b,q)$ distribution to the original data. $\small Z|X\sim Tulap(X,b,q)$.

```{r}
set.seed(1001)
#set up the parameters
n = 30                      #number of trials
truth = 0.4                 #true theta
reps = 100000               #sample size
ep = 1                      #epsilon
de = 0.01                   #delta
b = exp(-ep)                #b
q = 2*de*b/(1-b+2*de*b)     #q
alpha = 0.05                #level alpha

#create the orginal data though binomial simulation
X = rbinom(n = reps, size = n, prob = truth)

#add Tulap noise to the original 
#perform statistical testing on this data
Z = rep(NA, length(X))
Z = X + rtulap(n = reps, m = 0, b = b, q = q)
```

The summary of the first 10 observations from $\small X$ and $\small Z$ is as follows:
```{r tab1, results='asis', echo=FALSE, message=FALSE}
options(digits = 3)
tab = rbind(head(round(X, 0), 10), head(round(Z,2), 10))
rownames(tab) = c("X", "Z")
knitr::kable(tab)
```

The plot of the distribution of $X$ and $Z$ is as below:
```{r fig1, echo=FALSE, fig.width= 4.5, fig.height= 3.5, fig.align='center'}
#plot the distribution of the original data and the differentially private data
hist(X, freq = FALSE, main = "Data Distribution", xlab = "", breaks= seq(1.5, 24.5, by = 1))
lines(density(Z), col = "blue")
legend("topright", c("Original", "Tulap"), col = c("black", "blue"), lwd = 1)
```


## Differentially Private Uniformly Most Powerful Tests (DP-UMP)

### Simple and One-Sided DP-UMP Tests
For example, to calculate the one-sided left DP-UMP test for $\small H_0: \theta \le 0.4$ against $\small H_1: \theta > 0.4$:
```{r}
leftUMP = umpLeft(theta = 0.4, size = 10, alpha = 0.05, epsilon = ep, delta = de)
rightUMP = umpRight(theta = 0.4, size = 10, alpha = 0.05, epsilon = ep, delta = de)
```

```{r, echo = FALSE, fig.width = 7, fig.height = 4, fig.align = 'center'}
par(mfrow=c(1, 2))
plot(leftUMP, type = "l", main = "One-sided Left DP-UMP Test", ylab = expression(phi(x)), xlab = "x")
plot(rightUMP, type = "l", main = "One-sided Right DP-UMP Test", ylab = expression(phi(x)), xlab = "x")
```

$\small \Phi(x)$ represents the probability of rejecting the null hypothesis when observing $X=x$.Unlike in the non-private setting where the test either results in rejection or acceptance, the randomness of the test in this case preserves privacy. 

As expected, the power of the left UMP is 0.05.
```{r}
dbinom(seq(0, 10),size = 10,prob = truth)%*%leftUMP
```

### Unbiased Two-Sided DP-UMP Tests
Following up from the previous example, we calculate the unbiased two-sided DP-UMP for $\small H_0: \theta= 0.4$ against $\small H_1: \theta \neq 0.4$.
```{r umpu}
twoUMP = UMPU(theta = 0.4, size = 10, alpha = 0.05, epsilon = ep, delta = de)
```

### Asymptotically Unbiased Two-Sided DP-UMP Tests
```{r}
numpu = umpuApprox(theta = 0.4, size = 10, alpha = 0.05, epsilon = ep, delta = de)
```

```{r fig4, echo= FALSE, results='asis', fig.width = 6, fig.height=4, fig.align = 'center'}
par(mfrow=c(1, 1))
plot(numpu, type = "l", lwd = 1.5, col = "red", main = "Comparing Unbiased DP-UMP Tests", ylab = expression(phi(x)), xlab = "x")
lines(twoUMP, type = "l", lwd = 1.5, lty = 2, col = "blue")
legend("topleft", legend=c("Asymptotically UMPU", "UMPU"), col=c("red", "blue"), lty=1:2, lwd = 1.5)
```


## p-Values

### UMP One-Sided p-Values
We continue to use the noisy data $\small Z$ to test the hypothesis $\small H_0: \theta \le 0.5$ against $\small H_A: \theta > 0.5$. As shown in the Data Simulation section, the true data $\small X$ is simulated under Binomial distristribution with $\small \theta = 0.4$, which is unknown in the real setting, and the noisy data $\small Z$ is obtained by adding $\small Tulap(0,b,q)$ noise to $\small X$.  

```{r}
pvalL = pvalLeft(Z, size = n, theta = 0.5, b = b, q = q)
pvalR = pvalRight(Z, size = n, theta = 0.5, b = b, q = q)
```

The p-values for the first 10 observations in $\small Z$ is summarized below:
```{r,results='asis', echo = FALSE}
options(digits=2)
tab = rbind(head(Z, 10), head(pvalL, 10), head(pvalR, 10))
rownames(tab) = c("Z","Left private p-value", "Right private p-value")
knitr::kable(tab)
```

It is clear that with these p-values, we fail to reject the null hypothesis at level $\small \alpha = 0.05$ for all 10 observations. Since the true value $\small \theta= 0.4$ is in the null hypothesis, this confirms that the p-value has calibrated type I error.

```{r}
mean(pvalR < 0.05)
```

### Asymptotically Unbiased DP p-Values
Following up from the example above, we now test $\small H_0:\theta = 0.5$ versus $\small H_A: \theta \neq 0.5$ at level $\small \alpha = 0.05$. This test is an approximation to the DP-UMPU, with calibrated type I error, but is only asymptotically unbiased.

```{r}
upval = pvalTwoSide(Z, size = n, theta = 0.5, b = b, q = q)
```

```{r, results='asis', echo=FALSE}
options(digits = 2)
tabpval2 = rbind(head(Z, 10), head(upval, 10))
rownames(tabpval2) = c("Z", "P-Value")
knitr::kable(tabpval2)
```

Thus, we only have enough evidence to reject the null hypothesis at level $\small \alpha = 0.05$ with the 8th and 9th observation.

### Comparing Private and Non-Private p-values
We first calculate the non-private p-values using the first observation in the original data $\small X$: 
```{r}
x = (X[1]/n-0.5)/sqrt(0.5^2/30)
pR = pnorm(x, mean = 0, sd = 1, lower.tail = FALSE)
```

Then we add Tulap noise to the true data and calculate the private p-values 100,000 times. The distribution of private p-values compared to the non-private one is shown in the plot below. The randomness of private p-values helps preserve privacy.

```{r, fig.width = 6, fig.height=4, fig.align = 'center'}
#calculate private pvalue of the first observation 100000 times
tulap = rep(X[1], 100000) + rtulap(n = 100000, m = 0, b = b, q = q)
ppval = pvalRight(tulap, n, 0.5, b, q)

#plot the distriution of private p-values vs. non-private p-value
hist(ppval, xlim = c(0,1), 
     main = "Distribution of Private vs Non-Private p-values for X[1]", 
     xlab = "p-values")
abline(v=pR, col = "red")
legend("topright", 
       legend=c(paste("Non-private p-value =", round(pR,2)), 
                paste("Avg private p-value =", round(mean(ppval), 2))), 
       col=c("red", "black"), lty=1, lwd = 1.5)
```

Similarly, we continue to compare private vs. non private two-sided p-values:
```{r, fig.width = 6, fig.height=4, fig.align = 'center'}
#calculate non-private two-sided p-values
p2 = 2*pnorm(abs(x), mean = 0, sd = 1, lower.tail = FALSE)

#calculate private two-sided p-values
ppval2 = pvalTwoSide(tulap, n, 0.5, b, q)

#plot and compare
hist(ppval2, xlim = c(0,1), 
     main = "Distribution of Private vs Non-Private Two-Sided p-values for X[1]", 
     xlab = "p-values", probability = TRUE)
abline(v= p2, col = "red")
legend("topright", 
       legend=c(paste("Non-private p-value =", round(p2,2)), 
                paste("Avg private p-value =", round(mean(ppval2), 2))), 
       col=c("red", "black"), lty=1, lwd = 1.5)


```

## Confidence Intervals

### One-sided Confidence Intervals
Here, we know the value of the first observation in X is 18 and its corresponding $\small \theta$ is 0.6. However, in the real setting, we only know the value of the first observation in the differentially private data Z is 18.4243, and we want to estimate the $\small 95\%$ confidence interval of $\small \theta$.

```{r}
CILower(alpha = 0.05, Z[1], size = n, b = b, q = q)
CIUpper(alpha = 0.05, Z[1], size = n, b = b, q = q)
```

The upper and lower CI for $\small \theta$ at level $\small \alpha=0.05$ given the first observation of Z is $\small(0.45,1)$ and $\small (0, 0.76)$, respectively. Similarly, we obtain the lower bounds of the one-sided confidence intervals from the first 10 observations from the data:

```{r, echo = FALSE}
CIone = rep(NA, length(Z))
for (i in 1:length(Z)){
  CIone[i] = CILower(alpha = 0.05, Z[i], size = n, b = b, q = q)
}
```

```{r, results='asis', echo=FALSE}
tab2 = rbind(head(Z,10), head(CIone, 10), rep(1,10))
rownames(tab2) = c("Z", "Lower Bound", "Upper Bound")
knitr::kable(tab2)
```

### Asymptotically Unbiased Two-sided Confidence Intervals
`CINearlyUnbiased()` computes the asymptotically unbiased two-sided confidence interval for $\small \theta$ with specified level of $\small \alpha$. This confidence interval is typically more accurate than the simple two-sided confidence interval. For example, to calculate the two-sided confidence interval of $\small \theta$ of the first observation in Z:

```{r}
CITwoSide(alpha = 0.05, Z[1], size = n, b = b, q = q)
```

Similarly, we obtain the $\small 95\%$ two-sided confidence intervals for the first 10 observations in Z:

```{r, results='hide'}
CITwoUpper = rep(NA, length(Z))
CITwoLower = rep(NA, length(Z))
for (i in 1:length(Z)){
  CITwoLower[i] = CITwoSide(alpha = 0.05, Z[i], size = n, b = b, q = q)[1]
  CITwoUpper[i] = CITwoSide(alpha = 0.05, Z[i], size = n, b = b, q = q)[2]
}
```

```{r, echo= FALSE, results='asis'}
tab1 = rbind(head(Z,10), head(CITwoLower, 10), head(CITwoUpper, 10))
rownames(tab1) = c("Z", "Lower Bound", "Upper Bound")
knitr::kable(tab1)
```

## Comparing Differentially Private and Non-Private Population Proportion Test
```{r}
#private test on noisy sample Z[1]
dpPropTest(Z[1], size = n, theta = 0.5, b = b, q = q, "greater", 0.05)

#non-private test on real data X[1]
prop.test(X[1], n, p = 0.5, "greater", conf.level = 0.95, correct = FALSE)
```


## References
Awan, Jordan Alexander, and Aleksandra Slavkovic. 2020. “Differentially Private Inference for Binomial Data”. Journal of Privacy and Confidentiality 10 (1). https://doi.org/10.29012/jpc.725.

Awan, Jordan, and Aleksandra Slavković. "Differentially private uniformly most powerful tests for binomial data." In Advances in Neural Information Processing Systems, pp. 4208-4218. 2018.
